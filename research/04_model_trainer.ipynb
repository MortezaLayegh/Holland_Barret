{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\layeg\\\\Desktop\\\\GitHub\\\\Holland_Barret\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\layeg\\\\Desktop\\\\GitHub\\\\Holland_Barret'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    model_name: str\n",
    "    n_estimators: int\n",
    "    target_column: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.XGBClassifier\n",
    "        schema =  self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path = config.train_data_path,\n",
    "            test_data_path = config.test_data_path,\n",
    "            model_name = config.model_name,\n",
    "            n_estimators = params.n_estimators,\n",
    "            target_column = schema.name\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from mlProject import logger\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "from mlProject.utils.common import read_yaml, create_directories, evaluate_clf\n",
    "\n",
    "#from mlProject.utils.common import evaluate_clf\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from imblearn.pipeline import  make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def gridsearch(models, X_train, y_train, X_test, y_test, preprocessor, balancer, param_grids,feature_names, kf, scoring, save_models=False, output_directory=None):\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        pipe = make_pipeline(preprocessor, balancer, model)\n",
    "\n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(pipe, param_grid =param_grids[model_name], cv=kf, scoring=scoring, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best model and its hyperparameters\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        classifier_step_key = model_name.lower()\n",
    "        print(best_model.named_steps.keys())\n",
    "        # Apply ColumnTransformer to the test data\n",
    "        X_test_transformed = best_model.named_steps['columntransformer'].transform(X_test)\n",
    "        # Use the transformed test data to make predictions using the classifier step\n",
    "        y_test_pred = best_model.named_steps[classifier_step_key].predict(X_test_transformed)\n",
    "        #y_test_pred = best_model.named_steps['columntransformer',classifier_step_key].predict(X_test) # avoid data leakage and sampling on test set by only using classifier step \n",
    "\n",
    "        # Evaluate Train and Test dataset\n",
    "        train_acc, train_f1 , train_precision, train_recall, train_roc_au = evaluate_clf(y_train, y_train_pred)\n",
    "        test_acc, test_f1 , test_precision, test_recall, test_roc_au = evaluate_clf(y_test, y_test_pred)\n",
    "\n",
    "        #store results in dictionary\n",
    "        results[model_name] = {\n",
    "            'Best Hyperparameters': best_params,\n",
    "            'Best Score': best_score,\n",
    "            'Train Accuracy': train_acc,\n",
    "            'Train F1 Score': train_f1,\n",
    "            'Train Precision': train_precision,\n",
    "            'Train Recall': train_recall,\n",
    "            'Train Roc Auc': train_roc_au,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Test F1 Score': test_f1,\n",
    "            'Test Precision': test_precision,\n",
    "            'Test Recall': test_recall,\n",
    "            'Test Roc Auc': test_roc_au\n",
    "        }\n",
    "\n",
    "        # Save the best model to the specified directory if specified\n",
    "        if save_models and output_directory:\n",
    "            current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_filename = f\"{model_name.replace(' ', '_')}_{current_datetime}.joblib\"\n",
    "            model_filepath = os.path.join(output_directory, model_filename)\n",
    "            joblib.dump(best_model, model_filepath)\n",
    "            results[model_name]['Saved Model Filepath'] = model_filepath\n",
    "\n",
    "                # For models that support feature importance or coefficients\n",
    "        if hasattr(best_model.named_steps[classifier_step_key], 'feature_importances_'):\n",
    "            results[model_name]['Feature Importances'] = dict(zip(feature_names, best_model.named_steps[classifier_step_key].feature_importances_))\n",
    "        elif hasattr(best_model.named_steps[classifier_step_key], 'coef_'):\n",
    "            results[model_name]['Coefficients'] = dict(zip(feature_names, best_model.named_steps[classifier_step_key].coef_))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.preprocessor_pipe =self.preprocessor_pipe()\n",
    "\n",
    "    \n",
    "    def preprocessor_pipe(self):\n",
    "        '''\n",
    "        Get data transformation object for preprocessing.\n",
    "        '''\n",
    "        # Define numerical and categorical features\n",
    "        train_data = pd.read_csv(self.config.train_data_path)\n",
    "        neededColumns = train_data.drop('Incomplete Transaction', axis=1)\n",
    "        num_features = neededColumns.select_dtypes(exclude=\"object\").columns\n",
    "        cat_features = neededColumns.select_dtypes(include=\"object\").columns\n",
    "\n",
    "        # Define a pipeline for processing numeric features\n",
    "        numeric_processor = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy='mean')),\n",
    "                (\"scaler\", StandardScaler())\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define a pipeline for processing categorical features\n",
    "        categorical_processor = Pipeline(\n",
    "            steps=[\n",
    "                (\"Imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Categorical columns: {cat_features}\")\n",
    "        logger.info(f\"Numerical columns: {num_features}\")\n",
    "\n",
    "        # Combine numeric and categorical processors\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"numerical\", numeric_processor, num_features),\n",
    "                (\"categorical\", categorical_processor, cat_features)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "\n",
    "    def initiate_model_trainer(self):\n",
    "        logger.info(\"Initiating model training\")\n",
    "        train_data = pd.read_csv(self.config.train_data_path)\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "\n",
    "        X_train = train_data.drop([self.config.target_column], axis=1)\n",
    "        X_test = test_data.drop([self.config.target_column], axis=1)\n",
    "        y_train = train_data[self.config.target_column]\n",
    "        y_test = test_data[self.config.target_column]\n",
    "\n",
    "\n",
    "        models = {\n",
    "            \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "            \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "            \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "            \"LogisticRegression\": LogisticRegression(),\n",
    "            \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "            \"XGBClassifier\": XGBClassifier(), \n",
    "            \"CatBoostClassifier\": CatBoostClassifier(verbose=False),\n",
    "            \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "            \"SVC\": SVC()\n",
    "        } \n",
    "\n",
    "        param_grids = {\n",
    "            \"RandomForestClassifier\": {\n",
    "                \"randomforestclassifier__n_estimators\": [100],\n",
    "                \"randomforestclassifier__max_depth\": [None],\n",
    "                \"randomforestclassifier__min_samples_split\": [2],\n",
    "                \"randomforestclassifier__min_samples_leaf\": [1],\n",
    "                \"randomforestclassifier__bootstrap\": [True]\n",
    "            },\n",
    "            \"DecisionTreeClassifier\": {\n",
    "                \"decisiontreeclassifier__max_depth\": [None],\n",
    "                \"decisiontreeclassifier__min_samples_split\": [2],\n",
    "                \"decisiontreeclassifier__min_samples_leaf\": [1]\n",
    "            },\n",
    "            \"GradientBoostingClassifier\": {\n",
    "                \"gradientboostingclassifier__n_estimators\": [100],\n",
    "                \"gradientboostingclassifier__learning_rate\": [0.1],\n",
    "                \"gradientboostingclassifier__max_depth\": [3],\n",
    "                \"gradientboostingclassifier__min_samples_split\": [2],\n",
    "                \"gradientboostingclassifier__min_samples_leaf\": [1]\n",
    "            },\n",
    "            \"LogisticRegression\": {\n",
    "                \"logisticregression__C\": [1],\n",
    "                \"logisticregression__penalty\": ['l2']\n",
    "            },\n",
    "            \"KNeighborsClassifier\": {\n",
    "                \"kneighborsclassifier__n_neighbors\": [5],\n",
    "                \"kneighborsclassifier__weights\": ['uniform'],\n",
    "                \"kneighborsclassifier__metric\": ['euclidean']\n",
    "            },\n",
    "            \"XGBClassifier\": {\n",
    "                \"xgbclassifier__n_estimators\": [100],\n",
    "                \"xgbclassifier__max_depth\": [3],\n",
    "                \"xgbclassifier__learning_rate\": [0.1],\n",
    "                \"xgbclassifier__subsample\": [0.8],\n",
    "                \"xgbclassifier__colsample_bytree\": [0.8],\n",
    "                \"xgbclassifier__reg_alpha\": [0.001],\n",
    "                \"xgbclassifier__reg_lambda\": [0.001]\n",
    "            },\n",
    "            \"CatBoostClassifier\": {\n",
    "                \"catboostclassifier__iterations\": [100],\n",
    "                \"catboostclassifier__learning_rate\": [0.1],\n",
    "                \"catboostclassifier__depth\": [6],\n",
    "                \"catboostclassifier__l2_leaf_reg\": [3]\n",
    "            },\n",
    "            \"AdaBoostClassifier\": {\n",
    "                \"adaboostclassifier__n_estimators\": [100],\n",
    "                \"adaboostclassifier__learning_rate\": [0.1]\n",
    "            },\n",
    "            \"SVC\": {\n",
    "                \"svc__C\": [1],\n",
    "                \"svc__kernel\": ['rbf'],\n",
    "                \"svc__gamma\": ['scale']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        preprocessor = self.preprocessor_pipe\n",
    "        balancer = RandomUnderSampler(random_state=42)\n",
    "        feature_names= X_train.columns\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        scoring ='f1'\n",
    "        save_models=True\n",
    "        output_directory =self.config.root_dir\n",
    "\n",
    "        logger.info(\"Initiating grid search for models\")\n",
    "        results =gridsearch(models=models,param_grids= param_grids,X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,preprocessor = preprocessor,\n",
    "                balancer = balancer,\n",
    "                feature_names= feature_names,\n",
    "                kf=kf,\n",
    "                scoring =scoring, \n",
    "                save_models=save_models, \n",
    "                output_directory= output_directory)\n",
    "\n",
    "\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-23 10:21:32,574: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-23 10:21:32,576: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-23 10:21:32,579: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-23 10:21:32,580: INFO: common: created directory at: artifacts]\n",
      "[2024-02-23 10:21:32,581: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2024-02-23 10:21:32,595: INFO: 2643503123: Categorical columns: Index(['Gender', 'Region', 'Marital Status', 'Education'], dtype='object')]\n",
      "[2024-02-23 10:21:32,596: INFO: 2643503123: Numerical columns: Index(['Total Items', 'Unique Items', 'Total Sales', 'Discounted Sales',\n",
      "       'Browsing Duration (minutes)', 'Number of Clicks', 'Age',\n",
      "       'Household Income', 'Loyalty Card', 'Loyalty Points',\n",
      "       'Discount Percentage', 'Unique Items per Total Item', 'Month'],\n",
      "      dtype='object')]\n",
      "[2024-02-23 10:21:32,597: INFO: 2643503123: Initiating model training]\n",
      "dict_keys(['columntransformer', 'randomundersampler', 'randomforestclassifier'])\n",
      "dict_keys(['columntransformer', 'randomundersampler', 'decisiontreeclassifier'])\n",
      "dict_keys(['columntransformer', 'randomundersampler', 'gradientboostingclassifier'])\n",
      "dict_keys(['columntransformer', 'randomundersampler', 'logisticregression'])\n",
      "dict_keys(['columntransformer', 'randomundersampler', 'kneighborsclassifier'])\n",
      "dict_keys(['columntransformer', 'randomundersampler', 'xgbclassifier'])\n",
      "dict_keys(['columntransformer', 'randomundersampler', 'catboostclassifier'])\n",
      "dict_keys(['columntransformer', 'randomundersampler', 'adaboostclassifier'])\n",
      "dict_keys(['columntransformer', 'randomundersampler', 'svc'])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    results = model_trainer_config.initiate_model_trainer()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    model_name: str\n",
    "    n_estimators: int\n",
    "    max_depth: int\n",
    "    learning_rate: int\n",
    "    random_state: int\n",
    "    scale_pos_weight: int\n",
    "    min_child_weight: int\n",
    "    subsample: int\n",
    "    target_column: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.XGBClassifier\n",
    "        schema =  self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path = config.train_data_path,\n",
    "            test_data_path = config.test_data_path,\n",
    "            model_name = config.model_name,\n",
    "            n_estimators = params.n_estimators,\n",
    "            max_depth= params.max_depth,\n",
    "            learning_rate= params.learning_rate,\n",
    "            random_state= params.random_state,\n",
    "            scale_pos_weight= params.scale_pos_weight,\n",
    "            min_child_weight= params.min_child_weight,\n",
    "            subsample= params.subsample,\n",
    "            target_column = schema.name\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from mlProject import logger\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "from mlProject.utils.common import read_yaml, create_directories, evaluate_clf\n",
    "\n",
    "#from mlProject.utils.common import evaluate_clf\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from imblearn.pipeline import  make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def initiate_model_trainer(self):\n",
    "        logger.info(\"Initiating model training\")\n",
    "        train_data = pd.read_csv(self.config.train_data_path)\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "\n",
    "        X_train = train_data.iloc[:, :-1]\n",
    "        X_test = test_data.iloc[:, :-1]\n",
    "        y_train = train_data.iloc[:, -1]\n",
    "        y_test = test_data.iloc[:, -1]\n",
    "\n",
    "\n",
    "\n",
    "        xgb = XGBClassifier( n_estimators=self.config.n_estimators, max_depth=self.config.max_depth, \n",
    "                            learning_rate=self.config.learning_rate, random_state=self.config.random_state,\n",
    "                            scale_pos_weight=self.config.scale_pos_weight, min_child_weight=self.config.min_child_weight, \n",
    "                            subsample=self.config.subsample)\n",
    "        \n",
    "        xgb.fit(X_train, y_train)\n",
    "\n",
    "        joblib.dump(xgb, os.path.join(self.config.root_dir, self.config.model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-23 13:05:14,116: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-23 13:05:14,118: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-23 13:05:14,121: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-23 13:05:14,122: INFO: common: created directory at: artifacts]\n",
      "[2024-02-23 13:05:14,123: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2024-02-23 13:05:14,124: INFO: 3392366906: Initiating model training]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer_config.initiate_model_trainer()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    model_name: str\n",
    "    n_estimators: int\n",
    "    max_depth: int\n",
    "    learning_rate: int\n",
    "    random_state: int\n",
    "    min_samples_split: int\n",
    "    subsample: int\n",
    "    min_samples_leaf: int\n",
    "    target_column: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.GBMClassifier\n",
    "        schema =  self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path = config.train_data_path,\n",
    "            test_data_path = config.test_data_path,\n",
    "            model_name = config.model_name,\n",
    "\n",
    "            n_estimators = params.n_estimators,\n",
    "            max_depth= params.max_depth,\n",
    "            learning_rate= params.learning_rate,\n",
    "            random_state= params.random_state,\n",
    "            subsample= params.subsample,\n",
    "            min_samples_split= params.min_samples_split,\n",
    "            min_samples_leaf= params.min_samples_leaf,\n",
    "\n",
    "            target_column = schema.name\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from mlProject import logger\n",
    "from sklearn.ensemble import  GradientBoostingClassifier\n",
    "\n",
    "import joblib\n",
    "from mlProject.utils.common import read_yaml, create_directories, evaluate_clf\n",
    "\n",
    "#from mlProject.utils.common import evaluate_clf\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from imblearn.pipeline import  make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def initiate_model_trainer(self):\n",
    "        logger.info(\"Initiating model training\")\n",
    "        train_data = pd.read_csv(self.config.train_data_path)\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "\n",
    "        X_train = train_data.iloc[:, :-1]\n",
    "        X_test = test_data.iloc[:, :-1]\n",
    "        y_train = train_data.iloc[:, -1]\n",
    "        y_test = test_data.iloc[:, -1]\n",
    "\n",
    "        GBM = GradientBoostingClassifier(n_estimators=self.config.n_estimators, max_depth=self.config.max_depth, \n",
    "                            learning_rate=self.config.learning_rate, random_state=self.config.random_state,\n",
    "                            subsample=self.config.subsample, min_samples_split=self.config.min_samples_split,\n",
    "                            min_samples_leaf=self.config.min_samples_leaf)\n",
    "\n",
    "\n",
    "\n",
    "        GBM.fit(X_train, y_train)\n",
    "\n",
    "        joblib.dump(GBM, os.path.join(self.config.root_dir, self.config.model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 16:09:17,992: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-27 16:09:17,993: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-27 16:09:17,996: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-27 16:09:17,996: INFO: common: created directory at: artifacts]\n",
      "[2024-02-27 16:09:17,998: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2024-02-27 16:09:17,998: INFO: 3115623685: Initiating model training]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer_config.initiate_model_trainer()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
