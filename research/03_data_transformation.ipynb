{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\layeg\\\\Desktop\\\\GitHub\\\\Holland_Barret\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\layeg\\\\Desktop\\\\GitHub\\\\Holland_Barret'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    preprocessor_obj_file_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            preprocessor_obj_file_path=config.preprocessor_obj_file_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlProject import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.data = self.feature_eng_data_transform()  # Call feature_eng_data_transform upon initialization\n",
    "\n",
    "    ## Note: You can add i want to create a new feature to the data and then split \n",
    "    # df['Discount Percentage'] = ((df['Total Sales'] - df['Discounted Sales']) / df['Total Sales']) * 100\n",
    "    def feature_eng_data_transform(self):\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "\n",
    "        data['Discount Percentage'] = ((data['Total Sales'] - data['Discounted Sales']) / data['Total Sales']) * 100\n",
    "        data['Unique Items per Total Item'] = data['Unique Items'] / data['Total Items']\n",
    "        data['Month'] = pd.to_datetime(data['Date']).dt.month\n",
    "        logger.info(\"New feature created\")\n",
    "\n",
    "        data.drop(columns=['Customer ID', 'Transaction ID','Date'], inplace=True)\n",
    "        logger.info(\"Useless columns were dropped\")\n",
    "\n",
    "        data['Month'] = data['Month'].astype(str)\n",
    "        data['Loyalty Card'] = data['Loyalty Card'].astype(str)\n",
    "        logger.info(\"Data types of 'Month' and 'Loyalty Card' were changed to string\")\n",
    "        return data\n",
    "\n",
    "    def train_test_splitting(self):\n",
    "        X = self.data.drop('Incomplete Transaction', axis=1)\n",
    "        y = self.data['Incomplete Transaction']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "        '''\n",
    "        Get data transformation object for preprocessing.\n",
    "        '''\n",
    "\n",
    "        # Define numerical and categorical features\n",
    "        X = self.data.drop('Incomplete Transaction', axis=1)\n",
    "        num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "        cat_features = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "        # Define a pipeline for processing numeric features\n",
    "        numeric_processor = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy='mean')),\n",
    "                (\"scaler\", StandardScaler())\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define a pipeline for processing categorical features\n",
    "        categorical_processor = Pipeline(\n",
    "            steps=[\n",
    "                (\"Imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Categorical columns: {cat_features}\")\n",
    "        logger.info(f\"Numerical columns: {num_features}\")\n",
    "\n",
    "        # Combine numeric and categorical processors\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"numerical\", numeric_processor, num_features),\n",
    "                (\"categorical\", categorical_processor, cat_features)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return preprocessor\n",
    "    \n",
    "    def initiate_data_transformation(self):\n",
    "        preprocessing_obj = self.get_data_transformer_object()\n",
    "        X_train, X_test, y_train, y_test = self.train_test_splitting()\n",
    "        X_train = preprocessing_obj.fit_transform(X_train)\n",
    "        X_test = preprocessing_obj.transform(X_test)\n",
    "\n",
    "        # Combine input features with target feature\n",
    "        train_arr = np.c_[X_train, y_train]\n",
    "        test_arr = np.c_[X_test, y_test]\n",
    "\n",
    "        train_df = pd.DataFrame(train_arr)\n",
    "        test_df = pd.DataFrame(test_arr)\n",
    "\n",
    "        train_df.to_csv(os.path.join(self.config.root_dir, \"train_df.csv\"), index=False)\n",
    "        test_df.to_csv(os.path.join(self.config.root_dir, \"test_df.csv\"), index=False)\n",
    "\n",
    "        logger.info(\"data into training and test sets (scalled and imputed)\")\n",
    "        logger.info(X_train.shape)\n",
    "        logger.info(y_train.shape)\n",
    "\n",
    "        print(X_train.shape)\n",
    "        print(y_train.shape)\n",
    "\n",
    "        joblib.dump(preprocessing_obj, os.path.join(self.config.preprocessor_obj_file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-22 19:00:15,430: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-22 19:00:15,432: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-22 19:00:15,434: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-22 19:00:15,436: INFO: common: created directory at: artifacts]\n",
      "[2024-02-22 19:00:15,437: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-02-22 19:00:15,451: INFO: 3258793843: New feature created]\n",
      "[2024-02-22 19:00:15,453: INFO: 3258793843: Useless columns were dropped]\n",
      "[2024-02-22 19:00:15,457: INFO: 3258793843: Data types of 'Month' and 'Loyalty Card' were changed to string]\n",
      "[2024-02-22 19:00:15,460: INFO: 3258793843: Categorical columns: Index(['Gender', 'Region', 'Marital Status', 'Education', 'Loyalty Card',\n",
      "       'Month'],\n",
      "      dtype='object')]\n",
      "[2024-02-22 19:00:15,461: INFO: 3258793843: Numerical columns: Index(['Total Items', 'Unique Items', 'Total Sales', 'Discounted Sales',\n",
      "       'Browsing Duration (minutes)', 'Number of Clicks', 'Age',\n",
      "       'Household Income', 'Loyalty Points', 'Discount Percentage',\n",
      "       'Unique Items per Total Item'],\n",
      "      dtype='object')]\n",
      "[2024-02-22 19:00:15,580: INFO: 3258793843: data into training and test sets (scalled and imputed)]\n",
      "[2024-02-22 19:00:15,580: INFO: 3258793843: (4000, 29)]\n",
      "[2024-02-22 19:00:15,581: INFO: 3258793843: (4000,)]\n",
      "(4000, 29)\n",
      "(4000,)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.initiate_data_transformation()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
