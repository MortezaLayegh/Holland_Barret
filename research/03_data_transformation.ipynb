{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\layeg\\\\Desktop\\\\GitHub\\\\Holland_Barret\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\layeg\\\\Desktop\\\\GitHub\\\\Holland_Barret'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost data transformatio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for data transformation.\n",
    "\n",
    "    Attributes:\n",
    "    root_dir (Path): The root directory where transformed data will be stored.\n",
    "    data_path (Path): The path to the data to be transformed.\n",
    "    preprocessor_obj_file_path (Path): The file path of the preprocessor object.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    preprocessor_obj_file_path: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            preprocessor_obj_file_path=config.preprocessor_obj_file_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from mlProject import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "import joblib\n",
    "\n",
    "from imblearn.pipeline import  make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "class DataTransformation:\n",
    "    \"\"\"\n",
    "    Class for data transformation tasks.\n",
    "\n",
    "    Attributes:\n",
    "    config (DataTransformationConfig): The configuration for data transformation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initialize DataTransformation with a configuration object.\n",
    "\n",
    "        Args:\n",
    "        config (DataTransformationConfig): The configuration for data transformation.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.data = self.feature_eng_data_transform()  # Call feature_eng_data_transform upon initialization\n",
    "\n",
    "    def feature_eng_data_transform(self):\n",
    "        \"\"\"\n",
    "        Perform feature engineering on the data.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The transformed data.\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "\n",
    "        data['Discount Percentage'] = ((data['Total Sales'] - data['Discounted Sales']) / data['Total Sales']) * 100\n",
    "        data['Unique Items per Total Item'] = data['Unique Items'] / data['Total Items']\n",
    "        data['Month'] = pd.to_datetime(data['Date']).dt.month\n",
    "\n",
    "        data.drop(columns=['Customer ID', 'Transaction ID', 'Date'], inplace=True)\n",
    "\n",
    "        data['Month'] = data['Month'].astype(str)\n",
    "        data['Loyalty Card'] = data['Loyalty Card'].astype(str)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def train_test_splitting(self):\n",
    "        \"\"\"\n",
    "        Split the data into training and testing sets.\n",
    "\n",
    "        Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        X = self.data.drop('Incomplete Transaction', axis=1)\n",
    "        y = self.data['Incomplete Transaction']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.15, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "        \"\"\"\n",
    "        Get data transformation object for preprocessing.\n",
    "\n",
    "        Returns:\n",
    "        ColumnTransformer: Preprocessor object.\n",
    "        \"\"\"\n",
    "        X = self.data.drop('Incomplete Transaction', axis=1)\n",
    "        num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "        cat_features = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "        numeric_processor = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy='mean')),\n",
    "                (\"scaler\", StandardScaler())\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        categorical_processor = Pipeline(\n",
    "            steps=[\n",
    "                (\"Imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"numerical\", numeric_processor, num_features),\n",
    "                (\"categorical\", categorical_processor, cat_features)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def initiate_data_transformation(self):\n",
    "        \"\"\"\n",
    "        Perform data transformation and save the preprocessed data.\n",
    "\n",
    "        \"\"\"\n",
    "        preprocessing_obj = self.get_data_transformer_object()\n",
    "        X_train, X_test, y_train, y_test = self.train_test_splitting()\n",
    "        X_train = preprocessing_obj.fit_transform(X_train)\n",
    "        X_test = preprocessing_obj.transform(X_test)\n",
    "\n",
    "        balancer = RandomOverSampler(random_state=42)\n",
    "        X_train, y_train = balancer.fit_resample(X_train, y_train)\n",
    "\n",
    "        train_arr = np.c_[X_train, y_train]\n",
    "        test_arr = np.c_[X_test, y_test]\n",
    "\n",
    "        train_df = pd.DataFrame(train_arr)\n",
    "        test_df = pd.DataFrame(test_arr)\n",
    "\n",
    "        train_df.to_csv(os.path.join(self.config.root_dir, \"train_df.csv\"), index=False)\n",
    "        test_df.to_csv(os.path.join(self.config.root_dir, \"test_df.csv\"), index=False)\n",
    "\n",
    "        joblib.dump(preprocessing_obj, os.path.join(self.config.preprocessor_obj_file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-28 11:20:16,343: INFO: common: YAML file loaded successfully from: config\\config.yaml]\n",
      "[2024-02-28 11:20:16,344: INFO: common: YAML file loaded successfully from: params.yaml]\n",
      "[2024-02-28 11:20:16,347: INFO: common: YAML file loaded successfully from: schema.yaml]\n",
      "[2024-02-28 11:20:16,348: INFO: common: Created directory at: artifacts]\n",
      "[2024-02-28 11:20:16,350: INFO: common: Created directory at: artifacts/data_transformation]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize ConfigurationManager and retrieve data transformation configuration\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "\n",
    "    # Initialize DataTransformation with the retrieved configuration\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "\n",
    "    # Perform data transformation\n",
    "    data_transformation.initiate_data_transformation()\n",
    "\n",
    "except Exception as e:\n",
    "    # Raise the caught exception\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
